---
sidebar: sidebar 
permalink: task-segregate-snapmirror-azure.html 
keywords: segregate, SnapMirror, SnapMirror traffic, SnapMirror replication, add, additional NIC, new NIC, intercluster LIF, non-routable subnet, subnet 
summary: Azure의 Cloud Volumes ONTAP 사용하면 다른 네트워크를 사용하여 SnapMirror 복제 트래픽을 분리하여 데이터의 보안과 성능을 강화할 수 있습니다. 
---
= Azure에서 SnapMirror 트래픽 분리
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Azure의 Cloud Volumes ONTAP 사용하면 SnapMirror 복제 트래픽을 데이터 및 관리 트래픽에서 분리할 수 있습니다.  SnapMirror 복제 트래픽을 데이터 트래픽에서 분리하려면 새 네트워크 인터페이스 카드(NIC), 연관된 클러스터 간 LIF 및 라우팅이 불가능한 서브넷을 추가합니다.



== Azure의 SnapMirror 트래픽 분리에 관하여

기본적으로 NetApp Console 동일한 서브넷의 Cloud Volumes ONTAP 배포에 있는 모든 NIC와 LIF를 구성합니다.  이러한 구성에서는 SnapMirror 복제 트래픽과 데이터 및 관리 트래픽이 동일한 서브넷을 사용합니다.  SnapMirror 트래픽을 분리하면 데이터 및 관리 트래픽에 사용되는 기존 서브넷으로 라우팅할 수 없는 추가 서브넷을 활용할 수 있습니다.

.그림 1
다음 다이어그램은 단일 노드 배포에서 추가 NIC, 연관된 클러스터 간 LIF 및 라우팅 불가능한 서브넷을 사용하여 SnapMirror 복제 트래픽을 분리하는 방식을 보여줍니다.  HA 쌍 배포는 약간 다릅니다.

image:diagram-segregate-snapmirror-traffic.png["다이어그램은 단일 노드 구성에서 SnapMirror 복제 트래픽 분리를 보여줍니다."]

.시작하기 전에
다음 고려 사항을 검토하세요.

* SnapMirror 트래픽 분리를 위해 Cloud Volumes ONTAP 단일 노드 또는 HA 쌍 배포(VM 인스턴스)에 단일 NIC만 추가할 수 있습니다.
* 새로운 NIC를 추가하려면 배포하는 VM 인스턴스 유형에 사용되지 않는 NIC가 있어야 합니다.
* 소스 및 대상 클러스터는 동일한 가상 네트워크(VNet)에 액세스할 수 있어야 합니다.  대상 클러스터는 Azure의 Cloud Volumes ONTAP 시스템입니다.  소스 클러스터는 Azure의 Cloud Volumes ONTAP 시스템이나 ONTAP 시스템이 될 수 있습니다.




== 1단계: 추가 NIC를 생성하고 대상 VM에 연결합니다.

이 섹션에서는 추가 NIC를 생성하고 대상 VM에 연결하는 방법에 대한 지침을 제공합니다.  대상 VM은 Azure의 Cloud Volumes ONTAP 에 있는 단일 노드 또는 HA 쌍 시스템으로, 여기에 추가 NIC를 설정하려는 것입니다.

.단계
. ONTAP CLI에서 노드를 중지합니다.
+
[source, cli]
----
dest::> halt -node <dest_node-vm>
----
. Azure Portal에서 VM(노드) 상태가 중지되었는지 확인하세요.
+
[source, cli]
----
az vm get-instance-view --resource-group <dest-rg> --name <dest-vm> --query instanceView.statuses[1].displayStatus
----
. Azure Cloud Shell의 Bash 환경을 사용하여 노드를 중지합니다.
+
.. 노드를 중지합니다.
+
[source, cli]
----
az vm stop --resource-group <dest_node-rg> --name <dest_node-vm>
----
.. 노드의 할당을 해제합니다.
+
[source, cli]
----
az vm deallocate --resource-group <dest_node-rg> --name <dest_node-vm>
----


. 두 서브넷(소스 클러스터 서브넷과 대상 클러스터 서브넷)이 서로 라우팅되지 않도록 네트워크 보안 그룹 규칙을 구성합니다.
+
.. 대상 VM에 새 NIC를 만듭니다.
.. 소스 클러스터 서브넷의 서브넷 ID를 찾습니다.
+
[source, cli]
----
az network vnet subnet show -g <src_vnet-rg> -n <src_subnet> --vnet-name <vnet> --query id
----
.. 소스 클러스터 서브넷의 서브넷 ID를 사용하여 대상 VM에 새 NIC를 만듭니다.  여기에 새 NIC의 이름을 입력합니다.
+
[source, cli]
----
az network nic create -g <dest_node-rg> -n <dest_node-vm-nic-new> --subnet <id_from_prev_command> --accelerated-networking true
----
.. 개인IP주소를 저장합니다.  이 IP 주소 <new_added_nic_primary_addr>는 클러스터 간 LIF를 생성하는 데 사용됩니다.<<Step 2: Create a new IPspace,브로드캐스트 도메인, 새 NIC에 대한 클러스터 간 LIF>> .


. 새 NIC를 VM에 연결합니다.
+
[source, cli]
----
az vm nic add -g <dest_node-rg> --vm-name <dest_node-vm> --nics <dest_node-vm-nic-new>
----
. VM(노드)을 시작합니다.
+
[source, cli]
----
az vm start --resource-group <dest_node-rg>  --name <dest_node-vm>
----
. Azure Portal에서 *네트워킹*으로 이동하여 새 NIC(예: nic-new)가 있는지, 가속 네트워킹이 활성화되어 있는지 확인합니다.
+
[source, cli]
----
az network nic list --resource-group azure-59806175-60147103-azure-rg --query "[].{NIC: name, VM: virtualMachine.id}"
----


HA 쌍 배포의 경우 파트너 노드에 대해 단계를 반복합니다.



== 2단계: 새 NIC에 대한 새 IP 공간, 브로드캐스트 도메인 및 클러스터 간 LIF 만들기

클러스터 간 LIF를 위한 별도의 IP 공간은 클러스터 간 복제를 위한 네트워킹 기능 간의 논리적 분리를 제공합니다.

다음 단계에서는 ONTAP CLI를 사용하세요.

.단계
. 새로운 IPspace(new_ipspace)를 생성합니다.
+
[source, cli]
----
dest::> network ipspace create -ipspace <new_ipspace>
----
. 새로운 IPspace(new_ipspace)에 브로드캐스트 도메인을 만들고 nic-new 포트를 추가합니다.
+
[source, cli]
----
dest::> network port show
----
. 단일 노드 시스템의 경우 새로 추가된 포트는 _e0b_입니다. 관리형 디스크를 사용하는 HA 페어 배포의 경우 새로 추가된 포트는 _e0d_입니다. 페이지 블롭을 사용하는 HA 페어 배포의 경우 새로 추가된 포트는 _e0e_입니다. VM 이름이 아닌 노드 이름을 사용하십시오.  `node show`을 실행하여 노드 이름을 확인할 수 있습니다.
+
[source, cli]
----
dest::> broadcast-domain create -broadcast-domain <new_bd> -mtu 1500 -ipspace <new_ipspace> -ports <dest_node-cot-vm:e0b>
----
. 새로운 브로드캐스트 도메인(new_bd)과 새로운 NIC(nic-new)에 클러스터 간 LIF를 만듭니다.
+
[source, cli]
----
dest::> net int create -vserver <new_ipspace> -lif <new_dest_node-ic-lif> -service-policy default-intercluster -address <new_added_nic_primary_addr> -home-port <e0b> -home-node <node> -netmask <new_netmask_ip> -broadcast-domain <new_bd>
----
. 새로운 클러스터 간 LIF 생성을 확인합니다.
+
[source, cli]
----
dest::> net int show
----


HA 쌍 배포의 경우 파트너 노드에 대해 단계를 반복합니다.



== 3단계: 소스 시스템과 대상 시스템 간 클러스터 피어링 확인

이 섹션에서는 소스 시스템과 대상 시스템 간의 피어링을 확인하는 방법에 대한 지침을 제공합니다.

다음 단계에서는 ONTAP CLI를 사용하세요.

.단계
. 대상 클러스터의 클러스터 간 LIF가 소스 클러스터의 클러스터 간 LIF를 ping할 수 있는지 확인합니다.  대상 클러스터가 이 명령을 실행하므로 대상 IP 주소는 소스의 클러스터 간 LIF IP 주소입니다.
+
[source, cli]
----
dest::> ping -lif <new_dest_node-ic-lif> -vserver <new_ipspace> -destination <10.161.189.6>
----
. 소스 클러스터의 클러스터 간 LIF가 대상 클러스터의 클러스터 간 LIF를 ping할 수 있는지 확인합니다.  목적지는 목적지에 생성된 새로운 NIC의 IP 주소입니다.
+
[source, cli]
----
src::> ping -lif <src_node-ic-lif> -vserver <src_svm> -destination <10.161.189.18>
----


HA 쌍 배포의 경우 파트너 노드에 대해 단계를 반복합니다.



== 4단계: 소스 시스템과 대상 시스템 간 SVM 피어링 생성

이 섹션에서는 소스 시스템과 대상 시스템 간에 SVM 피어링을 생성하는 방법에 대한 지침을 제공합니다.

다음 단계에서는 ONTAP CLI를 사용하세요.

.단계
. 소스 클러스터 간 LIF IP 주소를 사용하여 대상에서 클러스터 피어링을 생성합니다. `-peer-addrs` .  HA 쌍의 경우 두 노드의 소스 클러스터 간 LIF IP 주소를 다음과 같이 나열합니다. `-peer-addrs` .
+
[source, cli]
----
dest::> cluster peer create -peer-addrs <10.161.189.6> -ipspace <new_ipspace>
----
. 암호를 입력하고 확인하세요.
. 대상 클러스터 LIF IP 주소를 사용하여 소스에서 클러스터 피어링을 생성합니다. `peer-addrs` .  HA 쌍의 경우 두 노드 모두에 대한 대상 클러스터 간 LIF IP 주소를 다음과 같이 나열합니다. `-peer-addrs` .
+
[source, cli]
----
src::> cluster peer create -peer-addrs <10.161.189.18>
----
. 암호를 입력하고 확인하세요.
. 클러스터가 피어링되었는지 확인하세요.
+
[source, cli]
----
src::> cluster peer show
----
+
피어링이 성공하면 가용성 필드에 *사용 가능*이 표시됩니다.

. 목적지에 SVM 피어링을 생성합니다.  소스 SVM과 대상 SVM은 모두 데이터 SVM이어야 합니다.
+
[source, cli]
----
dest::> vserver peer create -vserver <dest_svm> -peer-vserver <src_svm> -peer-cluster <src_cluster> -applications snapmirror``
----
. SVM 피어링을 허용합니다.
+
[source, cli]
----
src::> vserver peer accept -vserver <src_svm> -peer-vserver <dest_svm>
----
. SVM이 피어링되었는지 확인하세요.
+
[source, cli]
----
dest::> vserver peer show
----
+
피어 스테이트 쇼*`peered` * 및 피어링 애플리케이션이 표시됩니다.*`snapmirror` *.





== 5단계: 소스 시스템과 대상 시스템 간에 SnapMirror 복제 관계 생성

이 섹션에서는 소스 시스템과 대상 시스템 간에 SnapMirror 복제 관계를 만드는 방법에 대한 지침을 제공합니다.

기존 SnapMirror 복제 관계를 이동하려면 새 SnapMirror 복제 관계를 만들기 전에 먼저 기존 SnapMirror 복제 관계를 해제해야 합니다.

다음 단계에서는 ONTAP CLI를 사용하세요.

.단계
. 대상 SVM에 데이터 보호 볼륨을 만듭니다.
+
[source, cli]
----
dest::> vol create -volume <new_dest_vol> -vserver <dest_svm> -type DP -size <10GB> -aggregate <aggr1>
----
. SnapMirror 정책과 복제 일정을 포함하는 대상에 SnapMirror 복제 관계를 만듭니다.
+
[source, cli]
----
dest::> snapmirror create -source-path src_svm:src_vol  -destination-path  dest_svm:new_dest_vol -vserver dest_svm -policy MirrorAllSnapshots -schedule 5min
----
. 대상에서 SnapMirror 복제 관계를 초기화합니다.
+
[source, cli]
----
dest::> snapmirror initialize -destination-path  <dest_svm:new_dest_vol>
----
. ONTAP CLI에서 다음 명령을 실행하여 SnapMirror 관계 상태를 확인합니다.
+
[source, cli]
----
dest::> snapmirror show
----
+
관계 상태는 다음과 같습니다. `Snapmirrored` 그리고 관계의 건강은 `true` .

. 선택 사항: ONTAP CLI에서 다음 명령을 실행하여 SnapMirror 관계에 대한 작업 기록을 확인합니다.
+
[source, cli]
----
dest::> snapmirror show-history
----


선택적으로 소스 및 대상 볼륨을 마운트하고, 소스에 파일을 쓰고, 볼륨이 대상에 복제되는지 확인할 수 있습니다.
